{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "### Where will a new guest book their first travel experience?\n",
    "\n",
    "# Dataset Description\n",
    "In this challenge, we are given a list of users along with their demographics, web session records, and some summary statistics. We are asked to predict which country a new user's first booking destination will be. All the users in this dataset are from the USA.\n",
    "\n",
    "There are 12 possible outcomes of the destination country: 'US', 'FR', 'CA', 'GB', 'ES', 'IT', 'PT', 'NL','DE', 'AU', 'NDF' (no destination found), and 'other'.\n",
    "\n",
    "### File descriptions\n",
    "\n",
    "- train_users.csv - the training set of users\n",
    "\n",
    "\n",
    "- test_users.csv - the test set of users \n",
    "`id: user id \n",
    "date_account_created: the date of account creation\n",
    "timestamp_first_active: timestamp of the first activity, note that it can be earlier than date_account_created or  date_first_booking because a user can search before signing up \n",
    "date_first_booking: date of first booking \n",
    "gender \n",
    "age \n",
    "signup_method \n",
    "signup_flow: the page a user came to signup up from \n",
    "language: international language preference \n",
    "affiliate_channel: what kind of paid marketing \n",
    "affiliate_provider: where the marketing is e.g. google, craigslist, other \n",
    "first_affiliate_tracked: whats the first marketing the user interacted with before the signing up \n",
    "signup_app \n",
    "first_device_type \n",
    "first_browser \n",
    "country_destination: this is the target variable you are to predict `\n",
    "\n",
    "\n",
    "- sessions.csv - web sessions log for users \n",
    "`user_id: to be joined with the column 'id' in users table \n",
    "action \n",
    "action_type \n",
    "action_detail \n",
    "device_type \n",
    "secs_elapsed `\n",
    "\n",
    "\n",
    "- countries.csv - summary statistics of destination countries in this dataset and their locations \n",
    "\n",
    "\n",
    "- age_gender_bkts.csv - summary statistics of users' age group, gender, country of destination \n",
    "\n",
    "\n",
    "# Evalutaion Criterion\n",
    "![Evaluation Criterion](evaluation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rehas/anaconda3/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Disable warnings from printing\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Draw inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read the data\n",
    "train_users = pd.read_csv('../Data/train_users.csv')\n",
    "test_users = pd.read_csv('../Data/test_users.csv')\n",
    "\n",
    "# Drop country for processing\n",
    "LABEL = 'country_destination'\n",
    "labels = train_users[LABEL].copy()\n",
    "train_users.drop(LABEL, inplace=True, axis=1)\n",
    "\n",
    "# Merge train and test users\n",
    "users = pd.concat((train_users, test_users), axis=0, ignore_index=True)\n",
    "\n",
    "# Drop id for processing\n",
    "ids = users[\"id\"].copy()\n",
    "users.drop(\"id\", inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null values\n",
    "\n",
    "Creating a new column which counts the number of null values in each record helps to distinguish '-1' as a special type of value (missing) compared to other values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fill null with -1 for missing values\n",
    "users.fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date Parsing\n",
    "\n",
    "- Dates cannot be used as such. They are parsed to extract date of the weekday, week number, month, month and year.\n",
    "- This is done for date_account_created and timestamp_first_active.\n",
    "- There is another date feature date_first_booking which is completely null for test users. So we can ignore it completely.\n",
    "- Also compute the difference between dates date_account_created and timestamp_first_active."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# timestamp_first_active pre-processing to get date\n",
    "users.timestamp_first_active = users.timestamp_first_active.apply(str)\n",
    "users.timestamp_first_active = users.timestamp_first_active.str[:8]\n",
    "\n",
    "# Convert strings to dates\n",
    "users.date_account_created = pd.to_datetime(users.date_account_created, format=\"%Y-%m-%d\")\n",
    "users.timestamp_first_active = pd.to_datetime(users.timestamp_first_active, format=\"%Y%m%d\")\n",
    "\n",
    "def _parse_date(time_dt):\n",
    "    return [time_dt.year, time_dt.month, time_dt.day, time_dt.weekday(), time_dt.isocalendar()[1]]\n",
    "\n",
    "def extract_dates_inplace(features, date_column):\n",
    "    extracted_vals = np.vstack(features[date_column].apply(\n",
    "        (lambda x: _parse_date(x))))\n",
    "    for i, period in enumerate(['year', 'month', 'day', 'weekday', 'weekno']):\n",
    "        features['%s_%s' % (date_column, period)] = extracted_vals[:, i]\n",
    "    features.drop(date_column, inplace=True, axis=1)\n",
    "\n",
    "# date_account_created processing\n",
    "extract_dates_inplace(users, \"date_account_created\")\n",
    "extract_dates_inplace(users, \"timestamp_first_active\")\n",
    "\n",
    "# Since date_first_booking is null for entire test data, we can simply drop it.\n",
    "users.drop(['date_first_booking'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age\n",
    "- Age has lot of missing values.\n",
    "- It also has lot of spurious values like < 14 or > 100.\n",
    "- Replacing all these with -1.\n",
    "- We can leave these as -1 or predict all the -1 age values using a classifier such as KNN based on other columns.\n",
    "- Age values < 2000 and > 1900 are very likely to be birth years. so estimate age by 2014 - age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Age is missing for values -1, and spurious values > 100 or < 14\n",
    "users.age[users.age > 100] = -1\n",
    "users.age[users.age < 14] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Features\n",
    "- affiliate_channel, affiliate_provider, first_affiliate_tracked, first_browser,                    first_device_type, gender, language, signup_app, signup_method, signup_flow are all the categorical features. They also contain missing values which have been replace by -1.\n",
    "- Next one hot encoding is carried out on each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CATEGORICAL_FEATURES = ['affiliate_channel', 'affiliate_provider',\n",
    "                        'first_affiliate_tracked', 'first_browser',\n",
    "                        'first_device_type', 'gender', 'language', 'signup_app',\n",
    "                        'signup_method', 'signup_flow']\n",
    "\n",
    "for feature in CATEGORICAL_FEATURES:\n",
    "    users = pd.concat((users, pd.get_dummies(users[feature], prefix=feature)), axis=1)\n",
    "    users = users.drop(feature, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save processed training and testing data.\n",
    "training = users[:train_users.shape[0]]\n",
    "testing = users[train_users.shape[0]:].reset_index().drop(\"index\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "Below is the implementation of a simple XGBoost model that predicts 5 highest likely countries for each user in the test dataset. This is done because of the evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "le = LabelEncoder()\n",
    "xgb = XGBClassifier(max_depth=6, learning_rate=0.3, n_estimators=25,\n",
    "                    objective='multi:softprob', subsample=0.5, colsample_bytree=0.5, seed=0)                  \n",
    "xgb.fit(training, le.fit_transform(labels))\n",
    "predictions = xgb.predict_proba(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Taking the 5 classes with highest probabilities\n",
    "test_ids = ids[train_users.shape[0]:].reset_index().drop(\"index\", axis=1)\n",
    "ids_result = []  #list of ids\n",
    "cts_results = []  #list of countries\n",
    "for i in range(len(test_ids)):\n",
    "    idx = test_ids.iloc[i]\n",
    "    ids_result += [idx] * 5\n",
    "    cts_results += le.inverse_transform(np.argsort(predictions[i])[::-1])[:5].tolist()\n",
    "\n",
    "#Generate submission\n",
    "submission = pd.DataFrame(np.column_stack((ids_result, cts_results)), columns=['id', 'country'])\n",
    "submission.to_csv('../Data/submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "With above minimal feature engineering and XGBoost model we get an NDCG@5 score of 0.86995 and a Kaggle ranking of 537/1500 on the private leaderboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Sessions Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read the sessions data\n",
    "sessions_data = pd.read_csv(\"../Data/sessions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get unique user ids to store results\n",
    "sessions_data_ids = sessions_data.loc[:, [\"user_id\"]].drop_duplicates().reset_index().drop(\"index\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we count number of sessions by each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get number of sessions of each user\n",
    "sessions_data_ids = sessions_data_ids.join(\n",
    "    sessions_data.groupby(['user_id'])[\"user_id\"].size().to_frame(),\n",
    "    on=\"user_id\")\n",
    "sessions_data_ids.rename(columns={0 : \"NumSessions\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action, action_type, action_detail and device_type processing\n",
    "- Next for each of the attributes action, action_type, action_detail and device_type, we want to see what kind of distributions they have for each user.\n",
    "- Specifically we count the different types of actions for each user and also compute mean and standard deviation of this distribution of counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get user ids and actions for action processing\n",
    "sessions_data_action = sessions_data.loc[:, [\"user_id\", \"action\"]]\n",
    "\n",
    "# Processing for Action attribute\n",
    "# Replace null values with 'NAN'\n",
    "sessions_data_action.action = sessions_data_action.action.fillna(\"NAN\")\n",
    "\n",
    "# Replace infrequent values with 'Other' to avoid overfitting and speed up computation\n",
    "THRESHOLD = 0.0005*sessions_data_action.shape[0]\n",
    "counts = sessions_data_action.action.value_counts()\n",
    "sessions_data_action.action = sessions_data_action.action.apply(lambda x: 'Other' if counts[x] < THRESHOLD else x)\n",
    "\n",
    "# Get counts of each action for each user\n",
    "unique_actions = sessions_data_action[\"action\"].unique()\n",
    "\n",
    "for act in unique_actions:\n",
    "    sessions_data_ids = sessions_data_ids.join(\n",
    "        sessions_data_action.loc[sessions_data_action.action == act, :].groupby(['user_id']).size().to_frame(),\n",
    "        on=\"user_id\")\n",
    "    sessions_data_ids.rename(columns={0 : \"action_\" + act}, inplace=True)\n",
    "    \n",
    "sessions_data_ids = sessions_data_ids.fillna(0)\n",
    "\n",
    "# Get mean and std of distribution of counts of actions for each user\n",
    "sessions_data_ids[\"NumActionsMean\"] = \\\n",
    "sessions_data_ids.loc[:, \"action_lookup\":\"action_similar_listings_v2\"].mean(axis=1)\n",
    "\n",
    "sessions_data_ids[\"NumActionsStd\"] = \\\n",
    "sessions_data_ids.loc[:, \"action_lookup\":\"action_similar_listings_v2\"].std(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get user ids and action_type for action_type processing\n",
    "sessions_data_action_type = sessions_data.loc[:, [\"user_id\", \"action_type\"]]\n",
    "\n",
    "# Processing for action_type attribute\n",
    "# Replace null values with 'NAN'\n",
    "sessions_data_action_type.action_type = sessions_data_action_type.action_type.fillna(\"NAN\")\n",
    "sessions_data_action_type.action_type = sessions_data_action_type.action_type.replace(\n",
    "    {\n",
    "        '-unknown-': 'NAN'\n",
    "    }\n",
    ")\n",
    "\n",
    "# Replace infrequent values with 'Other' to avoid overfitting and speed up computation\n",
    "THRESHOLD = 0.005*sessions_data_action_type.shape[0]\n",
    "counts = sessions_data_action_type.action_type.value_counts()\n",
    "sessions_data_action_type.action_type = \\\n",
    "sessions_data_action_type.action_type.apply(lambda x: 'Other' if counts[x] < THRESHOLD else x)\n",
    "\n",
    "# Get counts of each action_type for each user\n",
    "unique_actions = sessions_data_action_type[\"action_type\"].unique()\n",
    "\n",
    "for act in unique_actions:\n",
    "    sessions_data_ids = sessions_data_ids.join(\n",
    "        sessions_data_action_type.loc[sessions_data_action_type.action_type == act, :].groupby(\n",
    "            ['user_id']).size().to_frame(),\n",
    "        on=\"user_id\")\n",
    "    sessions_data_ids.rename(columns={0 : \"action_type_\" + act}, inplace=True)\n",
    "    \n",
    "sessions_data_ids = sessions_data_ids.fillna(0)\n",
    "\n",
    "# Get mean and std of distribution of counts of action_type for each user\n",
    "sessions_data_ids[\"NumActionTypeMean\"] = \\\n",
    "sessions_data_ids.loc[:, \"action_type_NAN\":\"action_type_Other\"].mean(axis=1)\n",
    "\n",
    "sessions_data_ids[\"NumActionTypeStd\"] = \\\n",
    "sessions_data_ids.loc[:, \"action_type_NAN\":\"action_type_Other\"].std(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Repeat procedure for action_detail\n",
    "# Get user ids and action_detail for action_detail processing\n",
    "sessions_data_action_detail = sessions_data.loc[:, [\"user_id\", \"action_detail\"]]\n",
    "\n",
    "# Processing for action_detail attribute\n",
    "# Replace null values with 'NAN'\n",
    "sessions_data_action_detail.action_detail = sessions_data_action_detail.action_detail.fillna(\"NAN\")\n",
    "sessions_data_action_detail.action_detail = sessions_data_action_detail.action_detail.replace(\n",
    "    {\n",
    "        '-unknown-': 'NAN'\n",
    "    }\n",
    ")\n",
    "\n",
    "# Replace infrequent values with 'Other' to avoid overfitting and speed up computation\n",
    "THRESHOLD = 0.005*sessions_data_action_detail.shape[0]\n",
    "counts = sessions_data_action_detail.action_detail.value_counts()\n",
    "sessions_data_action_detail.action_detail = \\\n",
    "sessions_data_action_detail.action_detail.apply(lambda x: 'Other' if counts[x] < THRESHOLD else x)\n",
    "\n",
    "# Get counts of each action_type for each user\n",
    "unique_actions = sessions_data_action_detail[\"action_detail\"].unique()\n",
    "\n",
    "for act in unique_actions:\n",
    "    sessions_data_ids = sessions_data_ids.join(\n",
    "        sessions_data_action_detail.loc[sessions_data_action_detail.action_detail == act, :].groupby(\n",
    "            ['user_id']).size().to_frame(),\n",
    "        on=\"user_id\")\n",
    "    sessions_data_ids.rename(columns={0 : \"action_detail_\" + act}, inplace=True)\n",
    "    \n",
    "sessions_data_ids = sessions_data_ids.fillna(0)\n",
    "\n",
    "# Get mean and std of distribution of counts of action_detail for each user\n",
    "sessions_data_ids[\"NumActionDetailMean\"] = \\\n",
    "sessions_data_ids.loc[:, \"action_detail_NAN\":\"action_detail_listing_reviews\"].mean(axis=1)\n",
    "\n",
    "sessions_data_ids[\"NumActionDetailStd\"] = \\\n",
    "sessions_data_ids.loc[:, \"action_detail_NAN\":\"action_detail_listing_reviews\"].std(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Repeat the procedure for device_type\n",
    "# Get user ids and device_type for device_type processing\n",
    "sessions_data_device_type = sessions_data.loc[:, [\"user_id\", \"device_type\"]]\n",
    "\n",
    "# Processing for device_type attribute\n",
    "# Replace null values with 'NAN'\n",
    "sessions_data_device_type.device_type = sessions_data_device_type.device_type.fillna(\"NAN\")\n",
    "sessions_data_device_type.device_type = sessions_data_device_type.device_type.replace(\n",
    "    {\n",
    "        '-unknown-': 'NAN'\n",
    "    }\n",
    ")\n",
    "\n",
    "# Replace infrequent values with 'Other' to avoid overfitting and speed up computation\n",
    "THRESHOLD = 0.005*sessions_data_device_type.shape[0]\n",
    "counts = sessions_data_device_type.device_type.value_counts()\n",
    "sessions_data_device_type.device_type = \\\n",
    "sessions_data_device_type.device_type.apply(lambda x: 'Other' if counts[x] < THRESHOLD else x)\n",
    "\n",
    "# Get counts of each action_type for each user\n",
    "unique_actions = sessions_data_device_type[\"device_type\"].unique()\n",
    "\n",
    "for act in unique_actions:\n",
    "    sessions_data_ids = sessions_data_ids.join(\n",
    "        sessions_data_device_type.loc[sessions_data_device_type.device_type == act, :].groupby(\n",
    "            ['user_id']).size().to_frame(),\n",
    "        on=\"user_id\")\n",
    "    sessions_data_ids.rename(columns={0 : \"device_type_\" + act}, inplace=True)\n",
    "    \n",
    "sessions_data_ids = sessions_data_ids.fillna(0)\n",
    "\n",
    "# Get mean and std of distribution of counts of device_type for each user\n",
    "sessions_data_ids[\"NumDeviceTypeMean\"] = \\\n",
    "sessions_data_ids.loc[:, \"device_type_Windows Desktop\":\"device_type_Tablet\"].mean(axis=1)\n",
    "\n",
    "sessions_data_ids[\"NumDeviceTypeStd\"] = \\\n",
    "sessions_data_ids.loc[:, \"device_type_Windows Desktop\":\"device_type_Tablet\"].std(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing for secs_elapsed\n",
    "- secs_elapsed is a numerical attribute (time in seconds for that session).\n",
    "- For each user we compute the average, standard deviation, median and skew of this distribution of secs_elapsed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get user ids and secs_elapsed for secs_elapsed processing\n",
    "sessions_data_secs_elapsed = sessions_data.loc[:, [\"user_id\", \"secs_elapsed\"]]\n",
    "sessions_data_secs_elapsed.secs_elapsed = sessions_data_secs_elapsed.secs_elapsed.fillna(0)\n",
    "\n",
    "# Get simple stats on secs_elapsed\n",
    "tmp = sessions_data_secs_elapsed.groupby('user_id').aggregate(\n",
    "    [\n",
    "        np.mean, np.std, np.median, stats.skew\n",
    "    ]\n",
    ")\n",
    "\n",
    "tmp.columns = [\"secs_elapsed_mean\", \"secs_elapsed_std\", \"secs_elapsed_median\", \"secs_elapsed_skew\"]\n",
    "\n",
    "sessions_data_ids = sessions_data_ids.join(tmp, on=\"user_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Free memory\n",
    "del sessions_data_action\n",
    "del sessions_data_action_detail\n",
    "del sessions_data_action_type\n",
    "del sessions_data_device_type\n",
    "del sessions_data_secs_elapsed\n",
    "del sessions_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null values\n",
    "- There are some users about whom we have no sessions information.\n",
    "- Joing the two datasets will give Null values for sessions attributes for these users.\n",
    "- So for each user, we keep track of total number of null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# combine ids back with users data\n",
    "users[\"user_id\"] = ids\n",
    "\n",
    "# Merging train-test with session data\n",
    "all_data = pd.merge(users, sessions_data_ids, how='left')\n",
    "all_data = all_data.drop(['user_id'], axis=1)\n",
    "\n",
    "#Missing features for samples without sesssion data.\n",
    "all_data = all_data.fillna(-2)\n",
    "\n",
    "# Count of all types of null \n",
    "all_data['NumAllNulls'] = (all_data < 0).astype(int).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save processed training and testing data.\n",
    "training = all_data[:train_users.shape[0]]\n",
    "testing = all_data[train_users.shape[0]:].reset_index().drop(\"index\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "Below is the implementation of a simple XGBoost model that predicts 5 highest likely countries for each user in the test dataset. This is done because of the evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "le = LabelEncoder()\n",
    "xgb = XGBClassifier(max_depth=6, learning_rate=0.3, n_estimators=25,\n",
    "                    objective='multi:softprob', subsample=0.5, colsample_bytree=0.5, seed=0)                  \n",
    "xgb.fit(training, le.fit_transform(labels))\n",
    "predictions = xgb.predict_proba(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Taking the 5 classes with highest probabilities\n",
    "test_ids = ids[train_users.shape[0]:].reset_index().drop(\"index\", axis=1)\n",
    "ids_result = []  #list of ids\n",
    "cts_results = []  #list of countries\n",
    "for i in range(len(test_ids)):\n",
    "    idx = test_ids.iloc[i]\n",
    "    ids_result += [idx] * 5\n",
    "    cts_results += le.inverse_transform(np.argsort(predictions[i])[::-1])[:5].tolist()\n",
    "\n",
    "#Generate submission\n",
    "submission = pd.DataFrame(np.column_stack((ids_result, cts_results)), columns=['id', 'country'])\n",
    "submission.to_csv('../Data/submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "Above Users + sessions + xgboost model gives us an NDCG@5 score of \t0.88324 and a Kaggle ranking of 232/1500 on the private leaderboard."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
